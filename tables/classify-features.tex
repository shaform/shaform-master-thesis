\begin{table}[h]
\centering
\begin{tabular}{|c|c|c|c|}
\hline

\bf Features            & \bf Precision & \bf Recall & \bf F1-Score \\ \hline
    P \& N              &     48.73     &     75.73  &     59.26    \\ \hline
    POS                 &     62.64     &     70.98  &     66.52    \\ \hline
    NUM                 &     46.80     & \bf 78.87  &     58.71    \\ \hline
    SKIPGRAM            &     69.20     &     72.79  &     70.93    \\ \hline
    All-P \& N          &     72.24     &     76.48  &     74.27    \\ \hline
    All-POS             &     72.43     &     76.59  &     74.43    \\ \hline
    All-NUM             &     71.86     &     73.85  &     72.83    \\ \hline
    All-SKIPGRAM        &     64.09     &     76.37  &     69.65    \\ \hline
    All                 & \bf 73.30     &     76.75  & \bf 74.97    \\


\end{tabular}
\caption{\label{t:classify-features} Performance of discourse linking
disambiguation for connectives by different features. This is done by
treating each connective as an instance then classify their discourse usage
by logistic regression. Aftwards, we rank each candidate by length and their
logistic probability and reject overlappped ones.}
\end{table}

% 0.0
% prec: 0.2377    recall: 0.8803  f1: 0.3742
% 0.1
% prec: 0.6002    recall: 0.8597  f1: 0.7066
% 0.2
% prec: 0.6502    recall: 0.8387  f1: 0.7323
% 0.3
% prec: 0.6821    recall: 0.815   f1: 0.7423
% 0.4
% prec: 0.7085    recall: 0.7918  f1: 0.7476
% 0.5
% 0.733     recall: 0.7675  f1: 0.7497
% 0.6
% 0.757     recall: 0.736   f1: 0.7462
% 0.7
% prec: 0.782     recall: 0.6997  f1: 0.7383
% 0.8
% prec: 0.8089    recall: 0.6445  f1: 0.717
% 0.9
% prec: 0.8416    recall: 0.5172  f1: 0.6399 
% 1.0
% prec: 1         recall: 0       f1: 0
