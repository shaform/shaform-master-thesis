%
%   Chapter Related Works
%
%   Yong-Siang Shih
%   R.O.C.104.07
%
\chapter{Related Work}
\label{c:related}

In this chapter, we firstly introduce some English discourse corpora and the
related discourse research in English. Afterwards, we continue with the Chinese
discourse corpora and the related Chinese research works.

\section{English Discourse Corpus}

Two popular English large-scale discourse corpora are available for researchers.

In the Rhetorical Structure Theory Discourse Treebank (RST-DT)~\citep{Carlson01building},
there are 385 \textit{Wall Street Journal (WSJ)} articles selected from
the Penn Treebank~\citep{marcus1993building}. These articiles were annotated under
the \textit{Rhetorical Structure Theory (RST)}~\citep{mann-thompson88}.
Each article was segmented into \textit{elementary discourse units (EDUs)}, and the
\textit{rhetorical relations} between these EDUs were annotated in a tree structure
manner.
Consider the example sentences (S~\ref{sent:rst})  from \cite{Carlson01building}.

\begin{sent}{sent:rst}{}
[Still, analysts don't expect the buy-back to significantly affect per-share earnings in the short
term.]\textsuperscript{16} [``The impact won't be that great,'']\textsuperscript{17}
[said Graeme Lid gerwood of First Boston Corp.]\textsuperscript{18}
[This is in part because of the effect]\textsuperscript{19}
[of having to average the number of shares outstanding,]\textsuperscript{21}
[she said.]\textsuperscript{20} [In addition,]\textsuperscript{22}
[Mrs. Lidgerwood said,]\textsuperscript{23}
[Norfolk is likely to draw down its cash initially]\textsuperscript{24}
[to finance the purchases]\textsuperscript{25}
[and thus forfeit some interest income.]\textsuperscript{26}
\end{sent}

The discourse annotations are shown in Figure~\ref{i:rst-tree}. Each relation is represented
by an internal node, while each EDU is represented by a leaf node. A discourse relation is
annotated between different spans of EDUs. For example, there is a \textit{elaboration-additional}
relation between (17-21) and (22-26). Each relation can either be binary or multi-child.
Each span was also labeled as either \textit{NUCLEUS} or \textit{SATELLITE} relative
to a relation depending on its salience. The hierarchy annotations enable the analysis of
the complex discourse structure.

% i:rst-tree
\input{figures/rst-tree}

Comparatively, the Penn Discourse Treebank 2.0 (PDTB2)~\citep{Prasad08thepenn}
is a much larger corpus. The annotations were done on 2159 articles from
the WSJ corpus of the Penn Treebank~\citep{marcus1993building}. They adopted a lexically-grounded
approach, annotating explicit relations signalled by connectives without constructing higher-level
structures as done in RST-DT. Therefore, the arguments do not span over long ranges. In fact,
more than 90\% of the arguments appear in the same sentence of the connective or the
exact immediately preceding sentence as pointed out by \cite{kong2014a}.
In addition, they annotated implicit relations for successive pairs of sentences.


\section{English Discourse Research}

Many groups have investigated different subtasks of English
discourse parsing on PDTB2. \cite{pitler2009using} trained a maximum
entropy classifier with various syntactic features to identify explicit discourse
connectives and achieved the F1 of 94.19\%. Additionally, they used
a Naive Bayes classifier to classify the relation type of each connective and
achieved the accuracy of 94.15\%. Such high performance was already comparable to human
annotator agreement. They have found that the degree of relation type ambiguity
for connectives are low in English. In fact, an accuracy of 93.67\% can be achieved
by using the strings of connectives as the only features. \cite{wellner2009sequence}
also experimented with constituency-based and dependency-based features to identify
explicit connectives with logistic regression classifier. In addition, he proposed
a sequential ranking model to jointly identify discourse connectives and their arguments.
He developed a dependency-based discourse parsing system. \cite{faiz2013identifying} combined
various feature sets to improve the results on connective identification. \cite{j2013disambig} pointed
out that while a few connectives constitute most of connective instances, most articles contain
at least one low-frequency connective. Therefore, using macro-average over different connectives
may be a better metric. They also shown that simple lexical features such as POS tags perform
well in this experimental setting when golden standard parsing tree is not available.

For argument extraction.
\cite{lin2014pdtb} build an end-to-end discourse parser for PDTB2.

As RST-DT provides hierarchy discourse structure annotations. There are also many
attempts to construct these discourse structures automatically. \cite{soricut2003sentence}
investigated sentence-level discourse parsing. They built discourse segmenter to
segment each sentence into EDUs using a statistical model with lexical and syntactic features.
They evaluated the model by the ability to recover EDU boundaries and achieved the F1 of
83.1\% and 84.7\% with automatically generated parsing tree and golden truth parsing tree respectively.
They also constructed a discourse parser and shown that near-human level performance can be achieved
if golden truth syntactic parsing tree and EDU segmentation were given. \cite{sporleder2005} focused
on non-hierarchical discourse chunking and produced comparable results without the use of
syntactic parsers. \cite{fisher2007utility} investigated whether the features derived from finite-state
systems are enough for discourse segmentation and shown that combining features from syntactic tree
can improve the performance. \cite{joty2012novel} applied an CKY-like parsing algorithm
to sentence-level discourse parsing. By avoiding a greedy approach, they were able to increase
their performance. \cite{li2014recursive} utilized deep learning approach for document-level 
discourse parsing. They recursively learn the representations for discourse units.


\section{Chinese Discourse Corpus}

Early days research

HIT-CDTB
CDTB
%DCTD

\section{Chinese Discourse Research}



fully explain several research
%In the past, researchers investigated English discourse connective identification
%on Penn Discourse Treebank~\cite{Prasad08thepenn}. Pitler and Nenkova~\shortcite{pitler2009using} 
%used syntactic features to deal with discourse usage and
%relation type ambiguities. Various features were
%experimented to improve the results~\cite{faiz2013identifying,j2013disambig}.
%Ghosh~\shortcite{ghosh2012end} and Lin et al.~\shortcite{lin2014pdtb}
%attempted to build an end-to-end discourse parser.
%
%T'sou et al.~\shortcite{t1998automatic,t1999applying,t2000enhancement} and Chan
%et al.~\shortcite{chan2000mining} investigated connective detection in Chinese texts
%as a part of a tagging system. Hu et al.~\shortcite{hu2009research}
%developed a fully automatic system to extract connective components.
%They removed words commonly used in non-discourse contexts from the dictionary
%and improved accuracy from 0.5866 to 0.8988.
%Hu et al.~\shortcite{hu2011research} dealt with linking ambiguity.
%The above work focused on relatively short sentence groups.
%Zhou~\shortcite{zhou2012cross} and Li, Carpuat et al.~\shortcite{li2014cross} employed
%cross-lingual information to deal with discourse usage and relation type ambiguity.
%Their reported F scores for discourse usage identification are 0.7933 and 0.7163, respectively.
%Li, Carpuat et al.~\shortcite{li2014cross} used 5-way classification to evaluate discourse usage
%and relation type classification together and obtained overall accuracy of 0.7020.

