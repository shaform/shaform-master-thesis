%
%   Chapter Experiments
%
%   Yong-Siang Shih
%   R.O.C.104.07
%
\chapter{Experiments}
\label{c:exp}

In this chapter, we discuss the evaluation for our system. Each component
is evaluated individually as well as within the pipeline system. For
statistical significance, we use Wilcoxon signed-ranks test~\citep{wilcoxon1945individual}
as suggested by \cite{demvsar2006statistical} at confidence level 0.05.
For each experiment, we select the best model (denoted by !), and
* is used to denote the scores that are significantly different.


\section{Discourse Usage Disambiguation}
\label{s:discourse-usage-exp}

In this section, we describe our investigation
on discourse usage disambiguation. We evaluate our models using
10-fold cross-validation. The 2,342 paragraphs are divided  into 10 splits
while keeping the distribution for the number of explicit relations
in each paragraph roughly equal. Most of the related works only
deal with connective components, so we also use the 2,131 annotated
components to evaluate our models.
The precision, recall, and F1 scores for the positive instances are computed
for each fold, and the averaged results are reported.

In Section~\ref{s:linking-exp}, we will also investigate whether solving linking
ambiguities could improve the discourse usage disambiguation. The results
are summarized, and comparison with other works is presented.


\subsection{Disambiguation on Component Level}

We firstly evaluate disambiguation on component level as specified in
Section~\ref{s:discourse-disambig-component}.  
Different types of word embeddings for
the VECTOR features are experimented. We use Logistic Regression
as our classifier with VECTOR as the only features. The results are
shown in Table~\ref{t:recognition-vectors}. 

\input{tables/recognition-vectors}

GLOVE is trained with the GloVe toolkit while SKIPGRAM and CBOW are trained
using word2vec as describe in Section~\ref{s:pn-corpus}. SKIPGRAM yields
the best results. We have tried concatenating different embeddings,
but the performance does not improve. Therefore, we will only use SKIPGRAM
for the remaining experiments.

We also use the same 10-fold to evaluate different features we proposed.
The results are shown in Table~\ref{t:recognition-features}. SIKIPGRAM
is the most powerful feature set. Combining all features yields the best results.

\input{tables/recognition-features}

We use Scikit-Learn library~\citep{scikit-learn} to experiment with
several learning models using all features. For each model, default parameters
are used without tuning.
Table~\ref{t:recognition-models} shows the results. The F1 score for Logistic
Regression is the highest.


\input{tables/recognition-models}

\subsection{Disambiguation on Connective Level}

We continue to  evaluate disambiguation on connective level as specified in
Section~\ref{s:discourse-disambig-connective}. For evaluation on component instances,
we take the resulting connective candidates, and construct 
the union of all connective components among these candidates.

Table~\ref{t:recognition-connective-features} shows the results with different features.
As there remain many overlapped candidates, the precision for connective instances
are relatively low compared to the precision for component instances. The best performance
for component instances (78.68\%) is slightly higher than that of disambiguation on component level
(78.64\%) in Table~\ref{t:recognition-features}, but the difference is insignificant.

%t:recognition-connective-features
\input{tables/recognition-connective-features}

We also experiment with different learning models and the results
in Table~\ref{t:recognition-connective-models} are similar to that of disambiguation on
component level.

%t:recognition-connective-models
\input{tables/recognition-connective-models}

\section{Connective Linking Disambiguation}
\label{s:linking-exp}

In this section, we discuss our investigation on linking ambiguities between connective
components. Evaluation is done by calculating the precision, recall, and F1 scores for the
1,813 connective instances.
The same 10-fold for paragraphs is used for the experiments.

\subsection{Connective Linking Disambiguation for Known Connective Components}

We start by evaluating linking disambiguation individually by assuming all correct
connective components are already known. The results are reported in
Table~\ref{t:linking-perfect-methods}.
We evaluate different ranking criteria such as scores predicted by
Logistic Regression, the lengths of the connective candidates, and the combination
of the two\footnote{For \textit{len+score}, the candidates are ranked by lengths first,
and the candidates that have the same length are ranked by their scores.}.
Ties are broken by the positions of the connective candidates.
Baseline models that only rank the candidates by their positions are also reported.

We found that the ambiguity among the components is low. The simple
baseline model already achieves an F1 of 87.97\%. The greedy
Algorithm~\ref{a:linking-resolve} could improve the baseline
model significantly, but once ranking is employed, it doesn't contribute much.

%t:linking-perfect-methods
\input{tables/linking-perfect-methods}


\cite{hu2011research} evaluated their algorithm on 99 sentences that have
linking ambiguity, and they correctly resolved all linking ambiguity for 98
sentences. To evaluate our model on sentence level, 52 paragraphs that have linking ambiguity
and are composed of only 1 sentence are selected. We evaluate our non-learning \textit{len+\$} model
directly on these 52 sentences and are able to correctly resolve all linking
ambiguity for 51 sentences. Our relatively simple greedy algorithm is
able to achieve similar results when the components are known.

\subsection{Connective Linking Disambiguation within the Pipeline System}

For practical applications, it's difficult to know which component is correct in
advance. Therefore, we also evaluate linking disambiguation within the pipeline
system. The results are shown in Table~\ref{t:linking-methods}. Unlike the case
when all components are known, accepting candidates without linking ambiguity actually
decreases the performance. The reason is that if the component itself is spurious,
even if it has unique linking, the unique connective candidate it generates
is still spurious.

In addition, alternative approaches using discourse usage
disambiguation on component level and connective
level as described in Section~\ref{s:discourse-disambig-component} and
Section~\ref{s:discourse-disambig-component} are compared.
Although it is not statistically significant, for both connective
instances and components instances, the best F1 scores for disambiguation on connective
level (\textit{\#+len+score}) are higher than that for disambiguation on component level
(\textit{@+len+score}).
In addition, it is also relatively computation-efficient as the same Logistic
Regression predictions are used for both discourse usage disambiguation and ranking.
Therefore, we choose the \textit{\#+len+score} model for the next stage of our
pipeline system.

%t:linking-methods
\input{tables/linking-methods}

Since discourse disambiguation is not directly optimized together with linking
disambiguation, we also try to adjust the threshold for the Logistic Regression
to eliminate the candidates when using disambiguation on connective level to see if
it affects the performance, and we found that our model
obtains similar F1 scores across different thresholds as
shown in Figure~\ref{i:classify-threshold}.

%i:classify-threshold
\input{figures/classify-threshold}

Finally, we summarize our results for discourse disambiguation for
connective component instances in Table~\ref{t:recognition-methods}. We also
compare with various related work. The first two models are discourse disambiguation
on component level and on connective level without linking disambiguation. The following
models eliminate additional candidates by resolving linking ambiguity.
Some spurious candidates are removed in this process. However, it also removes some
correct components, so the resulting F1 score does not improve much.

%t:recognition-methods
\input{tables/recognition-methods}

Results from \cite{li2015automatic} are listed as they use the
same dataset as us. The best results for Maximum Entropy and Decision Tree classifiers with
automatic parsing tree features are selected. We use the 12,526 connective components candidates
extracted by string matching with connective lexicon as our basis to compute the accuracy in order
to compare with their work. Our models perform significantly better by a large margin.

We also list the results from \cite{zhou2012cross} and \cite{li2014cross} on different
datasets. Though it's not directly comparable with our work, the performance for F1 scores is similar,
and we have better accuracy.


\section{Discourse Relation Type Disambiguation}

\subsection{Relation Type Disambiguation for Known Connectives}

We will first evaluate our models using 10-fold cross-validation with the 1,813
explicit connectives. We divide them into 10 splits
while keeping the distribution for the relation types roughly equal for
each fold, and use Logistic Regression to classify the relation types.
Two sets of experiments for the four top-level
categories and the second-level categories are examined. The second-level categories
have 17 types, but there are three types (inference, background, and evaluation)
that have less than 10 instances. Therefore, we eliminate these instances and only evaluate
our models on the remaining 14 types. The resulting dataset contains 1,803 explicit
connectives. The macro-averaged
precision, recall, and F1 scores for different relation types and the accuracy for
all instances are computed. Finally, the averaged results over
the 10 folds are reported in Table~\ref{t:sense-features}.

We examined whether the same features
for connective candidates specified in Section~\ref{s:connective-features} are
also useful for relation type disambiguation. In addition, we use the string
of the connective itself as a feature. While the NUM features have some
discriminative power for discourse usage disambiguation on connective level,
it does not help for relation disambiguation. SKIPGRAM is still the most powerful
feature set.

%t:sense-features
\input{tables/sense-features}

In Table~\ref{t:sense-types}, we show the performance for different relation types
using \textit{All-NUM} as features. For the second-level relation types, we only list
those types that have more than 10 instances. We could notice that the
number of instances affects the performance of the learning model. The
lesser the instances, the worse the performance.

%t:sense-types
\input{tables/sense-types}

We also evaluate on component instances to compare with \cite{li2015automatic}.
We are able to achieve higher performance on relation type classification.

%t:sense-word-types
\input{tables/sense-word-types}

\subsection{Relation Type Disambiguation within the Pipeline System}

We also evaluate discourse relation disambiguation within the pipeline system
using the \textit{\#+len+score} model.
For each connective candidate extracted, we predict its relation type using
Logistic Regression with \textit{All-NUM} as features.
The same 10-fold for paragraphs is used, and the precision, recall, and F1 scores
for positive instances are computed. The results are listed in
Table~\ref{t:sense-types-pipeline}. The micro-averaged F1 72.64\% can be seen
as the ability for our model to propose connective candidates along with the correct
relation type. Compared with the original F1 score 74.97\% disregarding relation types,
the performance only decreases a little, showing the effectiveness of our
model for relation type disambiguation.

%t:sense-types-pipeline
\input{tables/sense-types-pipeline}

\section{Connective Argument Extraction}

\subsection{Connective Argument Extraction for Known Connectives}

In this section, we discuss our evaluation on argument extraction when the
positions of connectives are already known.

We will use the same 10-fold for paragraphs for evaluation.
The precision, recall, and F1 scores for the argument boundaries are
computed for each fold. Additionally, we also compute the accuracy among the 
connectives. Each connective is treated as an instance, and we only count it
as correct if all argument boundaries for the given connective are correctly identified.
The averaged results over all folds are reported. Table~\ref{t:argument-features} shows
the performance using different feature sets.

While the best F1 score for connective boundaries is 78.48\%, the accuracy for
the connectives is only 40.74\%, showing a lot of spaces for
improvement. It means that for each connective, we are able to recover most of its
argument boundaries, but it's difficult to recover all of them correctly.

%t:argument-features
\input{tables/argument-features}

We compare our model with a simple baseline model. The model places an argument
boundary at the beginning of each segment that contains a connective component for
the current connective. In addition, it places an argument boundary at the
end of the segment that contains the last connective component. If the connective
has only one component, only two boundaries are placed, so we place an additional
boundary after the last boundary or before the first boundary if
the last boundary is already the end of the paragraph. The results are shown
in Table~\ref{t:argument-baseline}. Our model outperforms the baseline model
significantly.

%t:argument-baseline
\input{tables/argument-baseline}

\subsection{Error Analysis}

We continue to analyze the errors produced by our model.
Figure~\ref{i:argument-error-len} shows the distribution of the number
of arguments and the number of connective components for each explicit discourse
relation and the respective error rates measured by connective accuracy.
Our model can mostly only handle the relations
that only have two arguments. For more arguments, the error rates are almost
close to 1. We also examine whether it's important for
the number of arguments and the number of component to match as shown in
Figure~\ref{i:argument-error-len-c}, but no significant difference is observed.

%i:argument-error-len
\input{figures/argument-error-by-len}

The arguments for an explicit discourse relation span over a continuous argument interval
that starts from the first argument boundary and ends at the last argument boundary.
There exist four types of relationships between the real interval and the predicted
interval as shown in Figure~\ref{i:args-relationship}.

%i:args-relationship
\input{figures/args-relationship}

We examine the distribution of the errors for these types of relationships and the
results are shown in Figure~\ref{i:argument-error-type}. We found in most cases,
the interval predicted is either too wide or too narrow.
Only 139 error cases are caused entirely by the argument boundaries within the
interval. The difficulty to determine the argument interval originates from
the large variety for the ranges of explicit relations. For the relations that
are represented by higher nodes in the discourse tree,
the range is so wide that it could contain almost the entire paragraph.
On the other hand, some relations only span over one part of a sentence.

%i:argument-error-type
\input{figures/argument-error-by-type}

Intriguingly, there are only 63 \textit{intersect} cases. Further investigation
reveals that in most cases, at least one side of the interval is correctly
identified. This is shown in Figure~\ref{i:argument-error-position}. Out of
the 1074 error cases, there are only 164 cases that the both sides of the
interval are incorrect. The reason behind this is probably due to the fact that
the existence of a connective often gives strong hint on at least one side of
the interval. On the contrary, there is often no explicit indication on the boundary
of the other side of the interval.

%i:argument-error-position
\input{figures/argument-error-by-position}

\subsection{Connective Argument Extraction within the Pipeline System}

Once argument extraction is integrated into the pipeline system, our end-to-end
pipeline would be able to extract explicit relations for raw text. In particular,
it proposes a set of explicit relation candidates, where each relation contains
three items: (1) the positions of the connective, (2) the relation type, and
(3) the arguments. \textit{\#+len+score} is used for connective extraction, and
\textit{All-NUM} is used to disambiguate 4 top-level relation types.
We use the same 10-fold for paragraphs to evaluate our model using each relation
as an instance. The precision, recall, and F1 scores are computed for each fold,
and the averaged results are reported. Each explicit relation candidate is
counted as true positive only when the three items are all correctly
identified; otherwise, it is counted as false positive. We list the results
in Table~\ref{t:final-results}. We could see that due to the difficulty
of argument extraction, the performance has decreased significantly compared
to the previous stage. We additionally list the performance of end-to-end
parsers reported by \cite{lin2014pdtb} and \cite{kong2014a}. Their results
were also evaluated on the F1 for the relations extracted.
However, their works include the identification of implicit relations while
our system focuses only on explicit relations.
Also, as they used PDTB2, the language and annotation standard are different from
ours, so it's not actually directly comparable.

%t:final-results
\input{tables/final-results}
