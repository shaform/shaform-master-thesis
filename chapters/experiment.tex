%
%   Chapter Experiments
%
%   Yong-Siang Shih
%   R.O.C.104.07
%
\chapter{Experiments}
\label{c:exp}

In this chapter, we discuss the evaluation for our system. Each component
is evaluated individually as well as within the pipeline system. For
statistical significance, we use Wilcoxon signed-ranks test~\citep{wilcoxon1945individual}
as suggested by \cite{demvsar2006statistical} at confidence level 0.05.

\section{Discourse Usage Disambiguation}
\label{s:discourse-usage-exp}

In this section, we describe our investigation
on discourse usage disambiguation. We evaluate our models using
10-fold cross-validation. The 2,342 paragraphs are divided  into 10 splits
while keeping the distribution for the number of explicit relations
in each paragraph roughly equal. Most of the related works only
deal with connective components, so we will also use the 2,131 annotated
components to evaluate our models.
The precision, recall, and F1 score for the positive instances are computed
for each fold, and the averaged results are reported.

In Section~\ref{s:linking-exp}, we will also investigate whether solving linking
ambiguities could improve the discourse usage disambiguation. The results
are summarized, and comparison with with other works is presented.


\subsection{Disambiguation on Component Level}

We firstly evaluate disambiguation on component level as specified in
Section~\ref{s:discourse-disambig-component}.  
Different types of word embeddings for
the VECTOR features are experimented. We use Logistic Regression
as our classifier with VECTOR as the only features. The results are
shown in Table~\ref{t:recognition-vectors} below. 

\input{tables/recognition-vectors}

GLOVE is trained with the Glove toolkit~\citep{pennington2014glove}
while SKIPGRAM and CBOW are trained using word2vec~\citep{mikolov2013efficient} with
continuous skip-gram and continuous bag-of-words models,
respectively. SKIPGRAM yields the best results.
We have tried concatenating different embeddings
but the performance does not improve. Therefore, we will only use SKIPGRAM
for the remaining experiments.

We also use the same 10-fold to evaluate different features we proposed.
The results are shown in Table~\ref{t:recognition-features}. SIKIPGRAM
is the most powerful feature set. Combining all features yields the best results.

\input{tables/recognition-features}

We use Scikit-Learn library~\citep{scikit-learn} to experiment with
several learning models with default parameters using all features.
Table~\ref{t:recognition-models} shows the results. The F1 score for Logistic
Regression is the highest.


\input{tables/recognition-models}

\subsection{Disambiguation on Connective Level}

We continue to  evaluate disambiguation on connective level as specified in
Section~\ref{s:discourse-disambig-connective}. For evaluation on component instances,
we take the union of all connective components the resulting connective candidates
have as the final result for evaluation.

Table~\ref{t:recognition-connective-features} shows the results with different features.
As there remain many overlapped candidates, the precision for connective instances
are relatively low compared to the precision for component instances. The best performance
for component instances is slightly higher than that of disambiguation on component level in
Table~\ref{t:recognition-features},
but the difference is insignificant.

%t:recognition-connective-features
\input{tables/recognition-connective-features}

We also experiment with different learning models and the results
in Table~\ref{t:recognition-connective-models} are similar to that of disambiguation on
component level.

%t:recognition-connective-models
\input{tables/recognition-connective-models}

\section{Discourse Linking Disambiguation}
\label{s:linking-exp}

In this section, we discuss our investigation on linking ambiguities between connective
components. Evaluation is done by calculating the precision, recall, and F1 for the
1,813 connective instances.
The same 10-fold for paragraphs is used for the experiments.

\subsection{Discourse Linking Disambiguation for Known Connective Components}

We start by evaluating linking resolution individually by assuming all correct
connective components are already known. The results are reported in
Table~\ref{t:linking-perfect-methods}.
We evaluate different ranking criteria such as scores predicted by
Logistic Regression, the lengths of the connective candidates, and the combination
of the two. Ties are broken by the positions of the connective candidates.
Baseline models that only rank the candidates by their positions are also reported.

We found that the ambiguity among the components is low. The simple
baseline model already achieves an F1 of 87.97\%. The greedy
Algorithm~\ref{a:linking-resolve} could improve the baseline
model significantly, but once ranking is employed, it doesn't contribute much.
The difference for F1 between \textit{score+} and \textit{len+} is statistically
significant, but the difference between \textit{score+} and \textit{len-score+}
is insignificant.

%t:linking-perfect-methods
\input{tables/linking-perfect-methods}


\cite{hu2011research} evaluated their algorithm on 79 sentences that have
linking ambiguities, and they correctly resolved all linking ambiguities for 78
sentences. In our corpus, there are 52 paragraphs that have linking ambiguity
and are composed of only 1 sentence. We evaluate our non-learning \text{len+} model
directly on these 52 sentences and are able to correctly resolve all linking
ambiguities in 51 sentences. Our relatively simple greedy algorithm is
able to achieve similar results when the components are known.

\subsection{Discourse Linking Disambiguation within Pipeline System}

For practical applications, it's difficult to know which component is correct in
advance. Therefore, we also evaluate linking disambiguation within the pipeline
system. The results are shown in Table~\ref{t:linking-methods}. Unlike the case
when all components are known, accepting candidates with linking ambiguity $=$ 1 actually
decreases the performance.

In addition, alternative approaches using discourse usage
disambiguation on component level and connective
level as described in Section~\ref{s:discourse-disambig-component} and
Section~\ref{s:discourse-disambig-component} are compared.
Although it is not statistically significant, for both connective
instances and components instances, the best results for disambiguation on connective
level are higher to that for disambiguation on component level.
In addition, it is also relatively computation-efficient as the same Logistic
Regression predictions are used for both discourse usage disambiguation and ranking.
Therefore, we choose the \textit{\#len-score} model for the next stage of our
pipeline system.

%t:linking-methods
\input{tables/linking-methods}

Since discourse disambiguation is not directly optimized together with linking
disambiguation, we also try to adjust the threshold for the Logistic Regression
to eliminate the candidates when using disambiguation on connective level to see if
it affects the performance, and we found that our model
obtains similar F1 scores across different thresholds as
shown in Figure~\ref{i:classify-threshold}.

%i:classify-threshold
\input{figures/classify-threshold}

Finally, We summarize our results for discourse disambiguation for
connective component instances in Table~\ref{t:recognition-methods}. We also
compare with various related work. The first two models are discourse disambiguation
on component level and on connective level without linking resolution. The following
models eliminates additional candidates by resolving linking ambiguity.
Some spurious candidates are removed in this process. However, it also removes some
correct components, so the resulting F1 score does not improve much.

%t:recognition-methods
\input{tables/recognition-methods}

We also compare our results with the models from \cite{li2015automatic} as they use the
same dataset as us. The best results for Maximum Entropy and Decision Tree classifiers with
automatic parsing tree features are selected. We use the 12,526 connective components candidates
extracted by string matching with connective lexicon as our basis to compute the accuracy in order
to compare with their work. Our models perform significantly better by a large margin.

We also list the results from \cite{zhou2012cross} and \cite{li2014cross} on different
datasets. Though it's not directly comparable with our work, the performance for F1 is similar,
and we have better accuracy.


\section{Discourse Relation Disambiguation}

In this section, we discuss the evaluation on discourse relation disambiguation.

\subsection{Discourse Relation Disambiguation for Known Connectives}

We will first evaluate our models using 10-fold cross-validation with the 1,813
explicit connectives. We divide them into 10 splits
while keeping the distribution for the relation types roughly equal for
each fold. We use Logistic Regression to classify the relation types.
Two sets of experiments for the disambiguation on the four top-level
categories and the 17 second-level categories are examined. The macro-averaged
precision, recall, and F1 for different relation types and the accuracy for
all instances are computed. Since many of the 17 categories occur less than
10 times in the corpus, we ignore a relation type when computing macro-averaged
scores if it does not appear in the testing fold. The averaged results over
the 10 folds are shown in Table~\ref{t:sense-features}.

We examined whether the same features
for connective candidates specified in Section~\ref{s:connective-features} are
also useful for relation type disambiguation. In addition, we use the string
of the connective itself as a feature. While the NUM features have some
discriminative power for discourse usage disambiguation on connective level,
it does not help for relation disambiguation. SKIPGRAM is still the most powerful
feature set.

%t:sense-features
\input{tables/sense-features}

In Table~\ref{t:sense-types}, we show the performance for different relation types
using All-NUM as features. For the second-level relation types, we only list
those type that have more than 10 instances. We could notice that the
number of instances affects the performance of the learning model. The
lesser the instances, the worse the performance.

%t:sense-types
\input{tables/sense-types}

%t:sense-types-pipeline
\input{tables/sense-types-pipeline}

We also investigate end-to-end.

\section{Explicit Discourse Relation Argument Extraction}
