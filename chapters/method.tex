%
%   Chapter Methods
%
%   Yong-Siang Shih
%   R.O.C.104.07
%
\chapter{Methods}
\label{c:method}

\section{Connective Candidate Extraction}

\subsection{Goal}

To identify the correct discourse connectives, we need to
extract connective candidates and distinguish between discourse
and non-discourse usages among these candidates. Therefore, we tackle the
candidate identification first. In particular, we focus on the identification
of potential connective components, as connective candidates can then be
generated by forming links between these component candidates.

\subsection{Extraction Methods}

Given the input of a Chinese paragraph and the connective component lexicon we collected,
we would like to extract all possible positions of component candidates.
The simplest method is to just use string matching with the connective
component lexicon to extract all possible instances. This yields
24,539 candidates, while only 2,131 of them are correct instances. An improvement
could be made by making the observation that many of the components only have
discourse meaning when they are paired with other components. So instead of matching
with the connective component lexicon, we use connective lexicon and only extract
a component candidate when it forms a single connective or pairs with
other candidates to form a paired connective. This leaves us with 12,526 candidates.

One reason for many incorrect instances is that many characters used for connectives
appear in other unrelated words. For example, even though ``如'' (if) is a connective,
this character appears in an unrelated word ``如是说'' (says). To alleviate this problem,
Stanford Chinese segmenter~\citep{chang2008optimizing} is employed to segment paragraphs
into tokens. These token boundaries can be used as clues for eliminating spurious candidates.
In particular, we only extract a component when it can be composed by complete tokens.
For example, in (S~\ref{sent:keyissue}) we extract ``不是...而是'' (not ... but) as a candidate
even though ``不'' (not) and ``是'' (is) are separated. Comparatively, in (S~\ref{sent:improve}),
the correct connective ``如'' (if) is not extracted because it does not satisfy a token boundary.

\begin{sent}{sent:keyissue}{}
    当前 / 经济 / 的 / 关键 / \underline{不} / \underline{是} / 争取 / 更 /
    高 / 的 / 增长 / 速度 / ， / \underline{而是} / 提高 / 效益 / 。
    (The key issue for current economy is not to pursue faster growth, but to
    increase productivity.)
\end{sent}

\begin{sent}{sent:improve}{}
    \underline{如}无 / 改进 / ， / 很 / 难 / 在 / 海外 / 市场 / 参与 / 竞争 /
    。 (It will be difficult to compete in foreign markets if there is no
    improvement.)
\end{sent}

Using this procedure, only 7,649 component candidates are extracted, with 2,068 of 2,131 annotated
components recovered. These candidates could form 7,976 connective candidates, recovering
1,755 of 1,813 annotated connectives. Thus, 0.9704 and 0.9680 become upper bounds of recall
for the subsequent stages. Table~\ref{t:cand-extract} shows the comparison between different
extraction methods. The technique we use reduce spurious candidates substantially while
maintaining high recall.

%t:cand-extract
\input{tables/cand-extract}

\section{Discourse Usage Disambiguation}

\subsection{Goal}

We start by investigating the identification of connective components that have discourse functions
as most of the related works only deal with single connectives. Given the positions of all candidate
components, we would like to eliminate spurious candidates.

\subsection{Classification of Discourse Usage}

We use \textit{Scikit-Learn} library~\citep{scikit-learn} to carry out binary classification between discourse
and non-discourse usages for all candidates. Various classifiers such as \textit{Logistic Regression}
and \textit{Support Vector Machine (SVM)} have been experimented, and the results are
reported in Chapeter~\ref{c:exp}. The SVM included by Scikit-Learn is a \textit{Python} wrapper for the popular
\textit{LibSVM} library~\citep{CC01a}.

\subsection{Features}
\label{s:comp-features}

The feature set we used for classification is as follows.
When we extract features related to the parse tree, we use utilities provided by the
\textit{Natural Language Toolkit}~\citep{BirdKleinLoper09}.

\subsubsection{P\&N}

The feature set is a subset of the features from
\cite{pitler2009using}. It includes four binary features:
(1) the highest category that dominates exactly the component
itself in the parse tree, which is called self-category, (2) the parent of the self-category,
(3) the left-sibling of the self-category, and (4) the right-sibling of the self-category.
Special null features are set when no such nodes exist. For example,
in Figure~\ref{i:parse-but}, there is no node that dominates exactly the
connective component ``却是'' (but), while in Figure~\ref{i:parse-therefore} the
self-category of ``因此'' (therefore) is ADVP. We have also experimented
with the full feature set, but the performance does not increase.

%i:parse-but
%i:parse-therefore
\input{figures/parse}

\subsubsection{POS}

The feature set contains three types of binary features:
(1) part-of-speech tags for all tokens that constitute the connective component,
(2) part-of-speech tag of the token to the left of the component, and
(3) part-of-speech tag of the token to the right of the component. For example,
in (S~\ref{sent:sovereignty}) the features for ``意味着'' (meaning) are
\textbf{POS-left-PU}, \textbf{POS-right-PN}, \textbf{POS-self-VV}, and
\textbf{POS-self-AS}.

\begin{sent}{sent:sovereignty}{}
    一百二十七/CD 位/M 委员/NN 一致/AD 通过/VV ，/PU \underline{意味/VV 着/AS} 大家/PN 对/P
    恢复/VV 行使/VV 香港/NR 主权/NN 后/LC 的/DEG 政治/NN 架构/NN 重建/NN ，/PU
    有/VE 着/AS 高度/JJ 的/DEG 共识/NN 。/PU
    (All 127 committee members agreed, meaning that everyone reaches a consensus about
    the reconstruction of political structure after resuming exercise of sovereignty over Hong Kong.)
\end{sent}

\subsubsection{NUM}

The feature set contains three numerical features.

\begin{enumerate}
    \item The number of detected connective candidates the component involves. The larger
        the number means that the component candidate can form more plausible connective
        candidates by linking with other component candidates.
    \item The distance from the connective component to a separating element on the left.
    \item The distance from the connective component to a
        separating element on the right. Distance is measured by adding 1 to the number of tokens
        between the separating element and the nearest token of the component.
        The separating elements include the symbols ``！？：；，。''.
        When no such symbol is found, we use the distance to paragraph boundary.
\end{enumerate}

For example, in (S 3) the features for “意味着” (meaning) are \textbf{NUM-AMBIGUITY=X},
\textbf{NUM-left=1}, and \textbf{NUM-right=12}, where X depends on the number of candidate
connectives detected.


\subsubsection{VECTOR}

This feature set is built using word embeddings we created.
We have tried creating 400-dimensional embeddings by GloVe tool~\citep{pennington2014glove}
and word2vec tool~\citep{mikolov2013efficient,mikolov2013distributed}.

The vectors are used to construct three features: (1) the averaged vectors
for the tokens that constitute the connective component, (2) the vector for the
token to the left and (3) the vector for the token to the right. Zero-valued vector is used
when the vector does not exist.

In total, it's a 1200-dimensional vector when the embeddings are used independently.
The dimensionality increases proportionally when we combine different vectors produced by
different tools.


\section{Connective Linking Disambiguation}

\subsection{Goal}

Given all connective component candidates, we could construct all possible
connective candidates formed by these component. However, many of these
candidates are incorrect. In particular, a component may involves in
many different connective candidates by linking with different components,
but only one of them could be correct. Moreover, many component candidates
do not actually have discourse usage, meaning that none of the connectives
it forms is correct. We would like to extract the correct connective from
the paragraph.


\subsection{A Two-Stage Approach}

Rather than eliminating non-discourse connective components first and
dealing with linking disambiguation for overlapped connectives later,
we try to solve them together as linking may provide additional clues for
detecting discourse usage. We treat each connective candidate as an instance
and extract similar features for each candidate. Afterwards, a two-stage
process is carried out to identify correct instances.

Firstly, we use machine learning to eliminate unlikely candidates. This is done by
training a regression model to predict the scores for each candidate and eliminate
all instances that have scores under a threshold. When training, each real connective
is set to 1 and each spurious connective is set to 0. Since there are more spurious
connectives than real connectives, and we want to preserve most of the real connectives before
the next stage, we balance the data set by oversampling the correct instances for four times.  
At test time, the score of each candidate is obtained, and all
connectives with scores lower than a threshold are eliminated.

Afterwards, there remain some connective candidates that overlap with each other. That is,
they share the same connective component. So we greedily accept connective candidates
ordered by their lengths and then their scores, while rejecting overlapped ones. We have observed
that the more components a connective has, the less likely a spurious instance would be
detected. Thus we use length as a criterion for ordering.

\subsection{Features}

The feature sets for each connective candidate are similar to those in the previous
experiment:

\subsubsection{P\&N}

The feature set contains the union of self-category, parent-category, left-sibling,
and right-sibling as defined in Section~\ref{srcomp-features} for each component.
For example, in Figure~\ref{i:parse-although}, the features for ``虽'' (although) are
\textbf{PN-self-ADVP}, \textbf{PN-parent-VP}, \textbf{PN-left-Null}, and \textbf{PN-right-ADVP}, while
the features for ``但'' (but) are \textbf{PN-self-ADVP}, \textbf{PN-parent-IP}, \textbf{PN-left-Null},
and \textbf{PN-right-NP}. Therefore, the combined features for ``虽-但'' (although-but) are
\textbf{PN-self-ADVP}, \textbf{PN-parent-VP}, \textbf{PN-parent-IP}, \textbf{PN-left-Null},
\textbf{PN-right-ADVP}, and \textbf{PN-right-NP}.


%i:parse-although
\input{figures/parse-although}


\subsubsection{POS}

The feature set contains three types of binary features:
(1) part-of-speech tags for all tokens that constitute the connective components
of the connective candidate,
(2) part-of-speech tag of the token to the left of each component, and
(3) part-of-speech tag of the token to the right of each component. For example,
in (S~\ref{sent:crisis}), the features for ``虽-但'' (although-but) are
\textbf{POS-left-NR}, \textbf{POS-right-AD}, \textbf{POS-self-CS},
\textbf{POS-left-PU}, \textbf{POS-right-JJ}, and \textbf{POS-self-AD}.

\begin{sent}{sent:crisis}{}
    东亚/NR 诸国/NN 宛如/VV 骨牌/NN 般/VA 连番/AD 倒下/VV 。/PU
    台湾/NR \underline{虽/CS} 只/AD 遭/VV 风暴/NN 边缘/NN 扫过/VV ，/PU
    \underline{但/AD} 後续/JJ 冲击/NN 将/AD 一/CD 波波/M 涌到/VV 。/PU
    (East Asian countries fell like dominoes.
    Although Taiwan is only swept by the edge of the storm,
    the follow-up impact would come.
\end{sent}

\subsubsection{NUM}

The feature set contains the length of the connective as a set
of binary features. The length is defined as the number of components
that it contains.
In additional, there are seven numerical features.
\begin{enumerate}
    \item The number of overlapped connective candidates. Any connective
        candidates that share a token with any of the components for the current
        candidate is considered as overlapped.
    \item The number of connective candidates that cross any
        components of the current connective. A connective crosses a component
        if the component is between any two components of the connective.
    \item The distance between the leftmost and the rightmost tokens of the connective.
        The distance is measured by tokens as before.
    \item The geometric mean of distances between all neighboring connective components for
        The current connective candidates.
    \item The distances from the leftmost connective component to a separating element on the left.
    \item The distance from the rightmost connective component to a separating element on the right.
    \item The minimum distance from any separating element to any connective component.
\end{enumerate}

The separating elements include the symbols ``！？：；，。''.
When no such symbol is found, we use the distance to paragraph boundary.

\subsubsection{VECTOR}

This feature set is built using word embeddings as before, averaging over all components.
In particular, it contains three vectors: (1) the averaged vectors
for the all tokens that constitute the connective components, (2) the averaged vector for the
token to the left of each component and (3) the averaged vector for the token to the
right of each component. Zero-valued vector is used when the vector does not exist.

\section{Connective Argument Extraction}

\subsection{Goal}

\subsection{Argument Extraction as Sequence Labeling}

\subsection{Features}
