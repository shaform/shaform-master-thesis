%
%   Chapter Methods
%
%   Yong-Siang Shih
%   R.O.C.104.07
%
\chapter{Methods}
\label{c:method}

\section{Connective Candidate Extraction}

To identify the correct discourse connectives, we need to
extract connective candidates and distinguish between discourse
and non-discourse usages among these candidates.

Firstly, we attempt the detection of connective component candidates.
The simplest method is to just use string matching with the connective
component lexicon we obtained to extract all possible instances. This yields
24,539 candidates, while only 2,131 of them are correct instances. An improvement
could be made by making the observation that many of the components only have
discourse meaning when they are paired with other components. So instead of matching
with the connective component lexicon, we use connective lexicon and only extract
a component candidate when it forms a single connective or pairs with
other candidates to form a paired connective. This leaves us with 12,526 candidates.

One reason for many incorrect instances is that many characters used for connectives
appear in other unrelated words. For example, even though ``如'' (if) is a connective,
this character appears in an unrelated word ``如是说'' (says). To alleviate this problem,
Stanford Chinese segmenter~\citep{chang2008optimizing} is employed to segment paragraphs
into tokens. These token boundaries can be used as clues for eliminating spurious candidates.
In particular, we only extract a component when it can be composed by complete tokens.
For example, in (S~\ref{sent:keyissue}) we extract ``不是...而是'' (not ... but) as a candidate
even though ``不'' (not) and ``是'' (is) are separated. Comparatively, in (S~\ref{sent:improve}),
the correct connective ``如'' (if) is not extracted because it does not satisfy a token boundary.

\begin{sent}{sent:keyissue}{}
    当前 / 经济 / 的 / 关键 / \underline{不} / \underline{是} / 争取 / 更 /
    高 / 的 / 增长 / 速度 / ， / \underline{而是} / 提高 / 效益 / 。
    (The key issue for current economy is not to pursue faster growth, but to
    increase productivity.)
\end{sent}

\begin{sent}{sent:improve}{}
    \underline{如}无 / 改进 / ， / 很 / 难 / 在 / 海外 / 市场 / 参与 / 竞争 /
    。 (It will be difficult to compete in foreign markets if there is no
    improvement.)
\end{sent}

Using this procedure, only 7,649 component candidates are extracted, with 2,068 of 2,131 annotated
components recovered. These candidates could form 7,976 connective candidates, recovering
1,755 of 1,813 annotated connectives. Thus, 0.9704 and 0.9680 become upper bounds of recall
for the subsequent stages. Table~\ref{t:cand-extract} shows the comparison between different
extraction methods. The technique we use reduce spurious candidates substantially while
maintaining high recall.

%t:cand-extract
\input{tables/cand-extract}

\section{Discourse Usage Disambiguation}

We start by investigating the identification of connective components that have discourse functions.
We use \textit{Scikit-Learn} library~\citep{scikit-learn} to carry out binary classification between discourse
and non-discourse usages for all candidates. Various classifiers such as \textit{Logistic Regression}
and \textit{Support Vector Machine (SVM)} have been experimented, and the results are
reported in Chapeter~\ref{c:exp}. The SVM included by Scikit-Learn is a \textit{Python} wrapper for the popular
\textit{LibSVM} library~\citep{CC01a}.

\subsection{Features}

The feature set we used for classification is as follows.
When we extract features related to the parse tree, we use utilities provided by the
\textit{Natural Language Toolkit}~\citep{BirdKleinLoper09}.

\subsubsection{P\&N}

The feature set is a subset of the features from
\cite{pitler2009using}. It includes four binary features:
(1) the highest category that dominates exactly the component
itself in the parse tree, which is called self-category, (2) the parent of the self-category,
(3) the left-sibling of the self-category, and (4) the right-sibling of the self-category.
Special null features are set when no such nodes exist. For example,
in Figure~\ref{i:parse-but}, there is no node that dominates exactly the
connective component ``却是'' (but), while in Figure~\ref{i:parse-therefore} the
self-category of ``因此'' (therefore) is ADVP. We have also experimented
with the full feature set, but the performance does not increase.

%i:parse-but
%i:parse-therefore
\input{figures/parse}

\subsubsection{POS}

The feature set contains three types of binary features:
(1) part-of-speech tags for all tokens that constitute the connective component,
(2) part-of-speech tag of the token to the left of the component, and
(3) part-of-speech tag of the token to the right of the component. For example,
in (S~\ref{sent:sovereignty}) the features for ``意味着'' (meaning) are
\textbf{POS-left-PU}, \textbf{POS-right-PN}, \textbf{POS-self-VV}, and
\textbf{POS-self-AS}.

\begin{sent}{sent:sovereignty}{}
    一百二十七/CD 位/M 委员/NN 一致/AD 通过/VV ，/PU \underline{意味/VV 着/AS} 大家/PN 对/P
    恢复/VV 行使/VV 香港/NR 主权/NN 后/LC 的/DEG 政治/NN 架构/NN 重建/NN ，/PU
    有/VE 着/AS 高度/JJ 的/DEG 共识/NN 。/PU
    (All 127 committee members agreed, meaning that everyone reaches a consensus about
    the reconstruction of political structure after resuming exercise of sovereignty over Hong Kong.)
\end{sent}

\subsubsection{NUM}

The feature set contains three numerical features. (1) The number of detected
connective candidates the component involves. The larger the number of (1) means that
the component candidate can form more plausible connective candidates by linking with
other component candidates. (2) The distance from the connective component
to a separating element on the left. (3) The distance from the connective component to a
separating element on the right. Distance is measured by adding 1 to the number of tokens
between the separating element and the nearest token of the component.
The separating elements include the symbols ``！？：；，。''.
When no such symbol is found, we use the distance to paragraph boundary.
For example, in (S 3) the features for “意味着” (meaning) are \textbf{NUM-AMBIGUITY=X},
\textbf{NUM-left=1}, and \textbf{NUM-right=12}, where X depends on the number of candidate
connectives detected.


\subsubsection{VECTOR}

This feature set is built using word embeddings we created.
We have tried creating 400-dimensional embeddings by GloVe tool~\citep{pennington2014glove}
and word2vec tool~\citep{mikolov2013efficient,mikolov2013distributed}.

The vectors are used to construct three features: (1) the averaged vectors
for the tokens that constitute the connective component, (2) the vector for the
token to the right and (3) the vector for the token to the left. Zero-valued vector is used
when the vector does not exist.

In total, it's a 1200-dimensional vector when the embeddings are used independently.
The dimensionality increases proportionally when we combine different vectors produced by
different tools.



\section{Connective Linking Disambiguation}
\section{Connective Argument Extraction}
