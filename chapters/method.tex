%
%   Chapter Methods
%
%   Yong-Siang Shih
%   R.O.C.104.07
%
\chapter{Methods}
\label{c:method}

\section{Overview}

In this chapter, we discuss our system design for Chinese discourse analysis.
Our study mainly focuses on the analysis of explicit relations.
The stages for the pipeline system are shown in Figure~\ref{i:system}.

%i:system
\input{figures/system}

Firstly, we extract connective component candidates from each paragraph.
Secondly, a classifier is used to disambiguate between discourse and
non-discourse usages for these component candidates. We then try to identify
the correct connectives by resolving the linking ambiguity between the remaining
component candidates.
A different approach that eliminates non-discourse usages and disambiguates
linkings both on the connective level is also experimented. The alternative
approaches are illustrated in Figure~\ref{i:system-B}.
Finally, we determine the relation types for each explicit
connective and extract the arguments for each relation.

%i:system-B
\input{figures/system-B}


\section{Connective Candidate Extraction}

\subsection{Goal}

To identify the correct discourse connectives, we need to
extract connective candidates and distinguish between discourse
and non-discourse usages among these candidates. Therefore, we tackle the
candidate identification first. In particular, we focus on the identification
of potential connective components, since connective candidates can then be
generated by forming links between these component candidates.

\subsection{Extraction Methods}

Given the input of a Chinese paragraph and the connective component lexicon we collected,
we would like to extract all possible positions of component candidates.
The simplest method is to just use string matching with the connective
component lexicon to extract all possible instances. This yields
24,539 candidates, while only 2,131 of them are correct instances. An improvement
could be made by making the observation that many of the components only have
discourse meaning when they are paired with other components. So instead of matching
with the connective component lexicon, we use connective lexicon and only extract
a component candidate when it could independently form a single connective or pair
with other candidates to form a parallel connective.
This leaves us with 12,498 candidates.

One reason for many incorrect instances is that many characters used for connectives
appear in other unrelated words. For example, even though ``如'' (if) is a connective,
this character appears in an unrelated word ``如是说'' (says). To alleviate this problem,
Stanford Chinese segmenter~\citep{chang2008optimizing} is employed to segment paragraphs
into tokens. The boundaries of these tokens can be used as clues for eliminating spurious candidates.
In particular, we only extract a component when it can be composed of complete tokens.
For example, in (S~\ref{sent:keyissue}) we extract ``不是'' (not) and ``而是'' (but) as candidates
even though ``不'' (not) and ``是'' (is) are separated. Comparatively, in (S~\ref{sent:peace}),
the spurious connective component candidate ``和'' (and) is not extracted because it does
not satisfy a token boundary.

\begin{sent}{sent:keyissue}{}
    当前 / 经济 / 的 / 关键 / \underline{不} / \underline{是} / 争取 / 更 /
    高 / 的 / 增长 / 速度 / ， / \underline{而是} / 提高 / 效益 / 。
    (The key issue for current economy is not to pursue faster growth, but to
    increase productivity.)
\end{sent}

\begin{sent}{sent:peace}{}
    双方 / 表示 / 希望 / 在 / \underline{和}平 / 计划 / 的 / 基础 / 上 / 解决 / 问题 / 。
    (Both sides expressed the hope to solve the problem on the basis of the peace plan.)
\end{sent}

Using this procedure, only 7,649 component candidates are extracted, with 2,068 of 2,131 annotated
components recovered. These candidates could form 7,976 connective candidates, recovering
1,755 of 1,813 annotated connectives. Thus, 0.9704 and 0.9680 become upper bounds of recall
for the subsequent stages. Table~\ref{t:cand-extract} shows the comparison between different
extraction methods. The technique we use reduces the number of spurious candidates substantially while
maintaining high recall.

%t:cand-extract
\input{tables/cand-extract}

\section{Discourse Usage Disambiguation}
\label{s:discourse-disambig}

\subsection{Goal}

We start by investigating the identification of connective components that have discourse functions
as most of the related works also deal with connective components only.
Given the positions of all component candidates, we would like to eliminate spurious candidates.

\subsection{Disambiguation on Component Level}
\label{s:discourse-disambig-component}


We use \textit{Scikit-Learn} library~\citep{scikit-learn} to carry out binary
classification between discourse and non-discourse usages for all candidates.
Features for each connective component are extracted as specified in
Section~\ref{s:comp-features} and various
classifiers such as \textit{Logistic Regression} and
\textit{Support Vector Machine (SVM)}\footnote{The SVM included by Scikit-Learn is
a \textit{Python} wrapper for the popular \textit{LibSVM} library~\citep{CC01a}.}
have been experimented. The results are reported in Chapter~\ref{c:exp}. 
After we eliminate spurious components, we could form connective candidates
by linking the remaining components. Linking ambuirities must be resolved to
identify the correct connective afterwards.


\subsection{Features for a Connective Component Candidate}
\label{s:comp-features}

In this section we discuss the features we propose for each connective
component candidate.
Stanford POS tagger~\citep{toutanova2003feature} and Stanford Chinese
parser~\citep{levy2003harder} are used to create the POS tags and parsing trees
for the paragraphs. When we extract features related to the parsing tree, we use
utilities provided by the \textit{Natural Language Toolkit}~\citep{BirdKleinLoper09}
to manipulate the parsing trees. The features we used for classification
are as follows.

\subsubsection{P\&N}

The feature set is a subset of the features from
\cite{pitler2009using}. It includes four binary features:
(1) the highest category that dominates exactly the component
itself in the parsing tree, which is called self-category, (2) the parent of the self-category,
(3) the left-sibling of the self-category, and (4) the right-sibling of the self-category.
Special null features are set when no such nodes exist. For example,
in Figure~\ref{i:parse-but}, there is no node that dominates exactly the
connective component ``却是'' (but), while in Figure~\ref{i:parse-therefore} the
self-category of ``因此'' (therefore) is ADVP. We have also experimented
with the full feature set from \cite{pitler2009using}, but the performance
does not increase.

%i:parse-but
%i:parse-therefore
\input{figures/parse}

\subsubsection{POS}

The feature set contains three types of binary features:
(1) part-of-speech tags for all tokens that constitute the connective component,
(2) part-of-speech tag of the token to the left of the component, and
(3) part-of-speech tag of the token to the right of the component. For example,
in (S~\ref{sent:sovereignty}) the features for ``意味着'' (meaning) are
\textbf{POS-left-PU}, \textbf{POS-right-PN}, \textbf{POS-self-VV}, and
\textbf{POS-self-AS}.

\begin{sent}{sent:sovereignty}{}
    一百二十七/CD 位/M 委员/NN 一致/AD 通过/VV ，/PU \underline{意味/VV 着/AS} 大家/PN 对/P
    恢复/VV 行使/VV 香港/NR 主权/NN \underline{后/LC} 的/DEG 政治/NN 架构/NN 重建/NN ，/PU
    有/VE 着/AS 高度/JJ 的/DEG 共识/NN 。/PU
    (All 127 committee members agreed, meaning that everyone reaches a consensus about
    the reconstruction of political structure after resuming exercise of sovereignty over Hong Kong.)
\end{sent}

\subsubsection{NUM}

The feature set contains three numerical features.

\begin{enumerate}
    \item The number of detected connective candidates the component involves. The larger
        the number means that the component candidate can form more plausible connective
        candidates by linking with other component candidates.
    \item The distance from the connective component to a separating element on the left.
    \item The distance from the connective component to a
        separating element on the right.
\end{enumerate}

Distance is measured by adding 1 to the number of tokens
between the separating element and the nearest token of the component.
The separating elements include the symbols ``！？：；，。''.
When no such symbol is found, we use the distance to paragraph boundary.

For example, in (S~\ref{sent:sovereignty}) the features for “意味着” (meaning) are \textbf{NUM-ambiguity=1},
\textbf{NUM-left=1}, and \textbf{NUM-right=12}. As there is only one connective candidate that has the
component “意味着” (meaning), the linking ambiguity for the component is 1.
We normalize the numerical features by scaling each to zero mean and unit variance.


\subsubsection{VECTOR}

This feature set is built using word embeddings we created.
We have tried creating 400-dimensional embeddings by GloVe tool~\citep{pennington2014glove}
and word2vec tool~\citep{mikolov2013efficient,mikolov2013distributed}.

The vectors are used to construct three features: (1) the averaged vectors
for the tokens that constitute the connective component, (2) the vector for the
token to the left and (3) the vector for the token to the right. Zero-valued vector is used
when the vector does not exist.

In total, it's a 1200-dimensional vector when the 400-dimensional embeddings are used.
The dimensionality increases proportionally if we combine different vectors produced by
different tools.

\subsection{Disambiguation on Connective Level}
\label{s:discourse-disambig-connective}


Rather than disambiguate each component candidate, we could also treat each connective
candidate as an instance and tackle discourse usage disambiguation on the connective
level. All possible connective candidates are  generated by linking components with
each other. Features for each connective candidate are extracted as specified in
Section~\ref{s:connective-features} and binary
classification can be carried out to identify discourse usages.
Since there are more spurious connectives than real connectives, we balance the
data set by oversampling the correct instances for three times when training. To evaluate on
component level, we take the union of the components for all connective as the result.

By eliminating unlikely connective candidates, some linking ambiguities 
are also removed. In fact, if we can classify perfectly, we would have already
solved the linking ambiguities completely as only the correct connectives remain.
However, there may still exist some spurious candidates, some of which are
caused by incorrect component candidates, others caused by
incorrect linkings.
In particular, some overlapped connective candidates that share the same
connective component candidate remain. By resolving the linking
ambiguities, we will have more capacity to identify the correct connectives. This
work is described in Section~\ref{s:linking-disambig}.

\subsection{Features for a Connective Candidate}
\label{s:connective-features}

The feature sets for each connective candidate are similar to those in the previous
experiment. As a connective candidate is composed of one or more component candidates,
we extend some of the features to utilize linking relationships between the
components.

\subsubsection{P\&N}

The feature set contains the union of self-category, parent-category, left-sibling,
and right-sibling as defined in Section~\ref{s:comp-features} for each component.
For example, in Figure~\ref{i:parse-although}, the features for ``虽'' (although) are
\textbf{PN-self-ADVP}, \textbf{PN-parent-VP}, \textbf{PN-left-Null}, and \textbf{PN-right-ADVP}, while
the features for ``但'' (but) are \textbf{PN-self-ADVP}, \textbf{PN-parent-IP}, \textbf{PN-left-Null},
and \textbf{PN-right-NP}. Therefore, the combined features for ``虽-但'' (although-but) are
\textbf{PN-self-ADVP}, \textbf{PN-parent-VP}, \textbf{PN-parent-IP}, \textbf{PN-left-Null},
\textbf{PN-right-ADVP}, and \textbf{PN-right-NP}.


%i:parse-although
\input{figures/parse-although}


\subsubsection{POS}

The feature set contains three types of binary features:
(1) part-of-speech tags for all tokens that constitute the connective components
of the connective candidate,
(2) part-of-speech tag of the token to the left of each component, and
(3) part-of-speech tag of the token to the right of each component. For example,
in (S~\ref{sent:crisis}), the features for ``虽-但'' (although-but) are
\textbf{POS-left-NR}, \textbf{POS-right-AD}, \textbf{POS-self-CS},
\textbf{POS-left-PU}, \textbf{POS-right-JJ}, and \textbf{POS-self-AD}.

\begin{sent}{sent:crisis}{}
    东亚/NR 诸国/NN 宛如/VV 骨牌/NN 般/VA 连番/AD 倒下/VV 。/PU
    台湾/NR \underline{虽/CS} 只/AD 遭/VV 风暴/NN 边缘/NN 扫过/VV ，/PU
    \underline{但/AD} 後续/JJ 冲击/NN 将/AD 一/CD 波波/M 涌到/VV 。/PU
    (East Asian countries fell like dominoes.
    Although Taiwan is only swept by the edge of the storm,
    the follow-up impact would come.)
\end{sent}

\subsubsection{NUM}

The feature set contains the length of the connective as a set
of binary features. The length is defined as the number of components
that it contains.
In additional, there are seven numerical features.
\begin{enumerate}
    \item The number of overlapped connective candidates. Any connective
        candidates that share a token with any of the components for the current
        candidate are considered as overlapped.
    \item The number of connective candidates that cross any
        components of the current connective. A connective crosses a component
        if the component is between any two components of the connective.
    \item The distance between the leftmost and the rightmost tokens of the connective.
        The distance is measured by tokens as before.
    \item The geometric mean of distances between all neighboring connective components for
        the current connective candidates.
    \item The distances from the leftmost connective component to a separating element on the left.
    \item The distance from the rightmost connective component to a separating element on the right.
    \item The minimum distance from any separating element to any connective component.
\end{enumerate}

The separating elements include the symbols ``！？：；，。''.
When no such symbol is found, we use the distance to paragraph boundary.
We normalize the numerical features by scaling each to zero mean and unit variance.

\subsubsection{VECTOR}

This feature set is built using word embeddings as before, averaging over all components.
In particular, it contains three vectors: (1) the averaged vectors
for the all tokens that constitute each connective component the connective candidate has,
(2) the averaged vector for the
token to the left of each component and (3) the averaged vector for the token to the
right of each component. Zero-valued vector is used when the vector does not exist.

\section{Connective Linking Disambiguation}
\label{s:linking-disambig}

\subsection{Goal}

Depending on the method we choose for discourse usage disambiguation, we
either have a set of connective component candidates or a set of connective
candidates. Given all component candidates, we could construct all possible
connective candidates formed by these components by linking them together.
Many of these candidates are incorrect. In particular, a component may involves in
multiple different connective candidates by linking with different components,
but only one of them could be correct. Moreover, some remaining component
candidates may not actually have discourse usage, meaning that none of
the connectives it forms is correct. We would like to extract the
correct connectives from the paragraph despite the imperfect discourse usage
disambiguation.

\subsection{A Greedy Algorithm}

We propose a greedy algorithm to resolve linking ambiguities among a set of
connective candidates as outlined in Algorithm~\ref{a:linking-resolve}.
The algorithm filters the candidate set $C$ and produces a result set $A$ that
contains only non-overlapped connective candidates. On line~\ref{a:lnk:compute}
and \ref{a:lnk:check}, the algorithm computes the number of connective candidates
each component candidate involves in the remaining $C$, and checks if there exists
a component that does not have multiple possible linkings. When such a candidate
is not found, we rank all connective candidates under some criteria
and select the one with the highest priority.

As we will see in our experiments, accepting components that have no ambiguity
is especially helpful when the only task is to resolve linking ambiguities among
component candidates that are known to be correct.
In fact, under this condition, we can guarantee the connective candidate
$c_i$ found on line~\ref{a:lnk:select} is correct as long as $A$ does not yet have
any spurious connective candidates. Moreover, one could also develop a non-greedy
algorithm that guarantees to produce a result set that contains all component
candidates as done by \cite{hu2011research}. However, we consider that not knowing
which component candidate is correct is a more realistic experiment setting so
we will not go into that direction.

\begin{algorithm}
    \caption{Linking Resolution Algorithm}
    \label{a:linking-resolve}
    \begin{algorithmic}[1]
        \Require
            $C$: A set of connective candidates
        \Ensure
            $A$: A set of accepted connectives
        \State $A \gets \{\}$
        \Comment{Initialize $A$}
        \While{$C$ is not empty }
            \State Compute linking ambiguities for all connective components exist in $C$ \label{a:lnk:compute}
            \If{there exists a connective component $ cc_i $ that has linking ambiguity $=$ 1} \label{a:lnk:check}
                \State let $ c_i $ be the unique connective candidate the component involves \label{a:lnk:select}
            \Else
                \State Rank all connective candidates in $ C $
                \State let $ c_i $ be the connective candidate that have highest priority
            \EndIf
                \State $C \gets C - \{c_i\}$
                \State $A \gets A \cup \{c_i\}$
            \State Remove all connective candidates $ c_j \in C $ that overlap with $ c_i $
        \EndWhile
        \State \Return A
    \end{algorithmic}
\end{algorithm}

When we use Algorithm~\ref{a:linking-resolve} in a pipeline system,
there exist many component candidates in $C$ that do not have discourse meaning,
and accepting any component that has linking ambiguity $=$ 1 could easily
introduce many spurious connective candidates. Therefore, we would just rank all
candidates and use their priority to greedily accept candidates. The simplified
algorithm is outlined in Algorithm~\ref{a:linking-rank}.

\begin{algorithm}
    \caption{Linking Resolution Algorithm by Ranking Only}
    \label{a:linking-rank}
    \begin{algorithmic}[1]
        \Require
            $C$: A set of connective candidates
        \Ensure
            $A$: A set of accepted connectives
        \State $A \gets \{\}$
        \Comment{Initialize $A$}
        \State Rank all connective candidates in $ C $
        \While{$C$ is not empty }
            \State let $ c_i $ be the connective candidate that have highest priority
            \State $C \gets C - \{c_i\}$
            \State $A \gets A \cup \{c_i\}$
            \State Remove all connective candidates $ c_j \in C $ that overlap with $ c_i $
        \EndWhile
        \State \Return A
    \end{algorithmic}
\end{algorithm}

We will use different ranking criteria to evaluate our models. The experiments are
explained in Section~\ref{s:linking-exp}.

\begin{description}
\item[Score] We use the features described in Section~\ref{s:connective-features} to
    train a logistic regression classifier to predict whether a connective candidate
    has discourse meaning and use the probability obtained as its score.

\item[Length] We use the number of components each connective candidate has as its
    score. This is based on our observation: when a component candidate can
    pair with other components, it's less likely for it to be a single connective by
    itself.

\item[Position] The positions of the components a connective candidate has is used
    as a tie-breaker for the ranking algorithm. In particular, we accept the left-most
    candidate first.
\end{description}

We explain two alternative pipeline approaches that correspond to two different
discourse usage disambiguation as described in Section~\ref{s:discourse-disambig-component}
and \ref{s:discourse-disambig-connective}.

\subsubsection{A Pipeline Approach with Discourse Usage Disambiguation on Component Level}
\label{s:pipeline1}

After we have eliminated the non-discourse connective components as described
in Section~\ref{s:discourse-disambig-component}, we could
deal with liking ambiguities by determining how to correctly pair these
components together. We generate all possible connective candidates by
linking these components. These candidates form a candidate set $ C $ and the set is
fed to the linking resolving algorithm to obtain the final result $ A $.
To obtain the scores, we treat each candidate as an instance to
extract the related features as described in Section~\ref{s:connective-features}.
Afterwards, logistic regression is used to obtain the likelihood for a
candidate to be a correct connective.

After this procedure, additional connective components may have been eliminated if
they are unable to form a link with other candidates and is not a single connective.
Therefore, we also evaluate whether this procedure could improve the performance
for discourse usage identification on component level.


\subsubsection{A Pipeline Approach with Discourse Usage Disambiguation on Connective Level}
\label{s:pipeline2}

Rather than eliminating non-discourse connective components first and
dealing with linking disambiguation for overlapped connectives later,
we could also try to solve them both on connective level as linking may
provide additional clues for detecting discourse usage. 

In Section~\ref{s:discourse-disambig-component}, we generate all possible
connective candidates and eliminate the ones that are unlikely to be correct.
In the process, we eliminate some non-discourse candidates and some incorrect
linkings.

Afterwards, there remain some connective candidates that overlap with each other.
That is, they share the same connective component. So we feed these candidates
to the linking resolving algorithm to obtain the final result. Since each candidate
already has a predicted score when disambiguating the discourse usage, the score
is used directly.

Likewise, we also evaluate on component level to see if this increases
the performance for discourse usage detection.

\section{Relation Type Disambiguation}

After the connectives are extracted, the relation type each connective represents
must be determined. Different classifiers are used to investigate whether the features in
Section~\ref{s:connective-features} are useful for relation type disambiguation.
Additionally, we also use the string of the connective as a feature as it provides strong
clues for the connective's relation type.

\section{Connective Argument Extraction}

\subsection{Goal}

After the connective and its relation type is identified, we would like to extract
its arguments. For example, in Figure~\ref{i:cdtb-tree2}, the discourse tree
for (S~\ref{sent:rare-earth}) is shown. There exist two explicit relations signalled by
connectives ``其中'' (among them) and ``并'' (and). We would like to know the
arguments for ``其中'' (among them) are (b) and (c) while arguments for ``并'' (and) are
(d), (e), (f), (g) and (h).

%i:cdtb-tree2
\input{figures/cdtb-tree2}

\begin{sent}{sent:rare-earth}{}
    [科技攻关带动了包头稀土工业生产的迅速发展。]\textsuperscript{a}
    [现已建成包钢稀土一、二、三厂等稀土生产厂家，]\textsuperscript{b}
    [\underline{其中}有世界最大的稀土精矿、稀土合金生产厂。]\textsuperscript{c}
    [目前包头可以生产八十多种稀土产品，]\textsuperscript{d}
    [这些产品以其独特的优良性能畅销全国，]\textsuperscript{e}
    [\underline{并}出口十几个国家和地区，]\textsuperscript{f}
    [年产值和利税达二十多亿元和三亿多元，]\textsuperscript{g}
    [出口值达五千多万美元。]\textsuperscript{h}
    (Science and technology research led to the rapid development of industrial production
    for rare earth minerals in Baotou. Baotou Steel Rare Earth has now completed three
    plants for rare earth production. Among them are the world's largest rare earth
    concentrate and rare earth alloy production plant. Baotou can now produce more than
    eighty kinds of rare earth products. With its excellent properties, these products
    are sold nationwide and exported to more than 10 countries and regions.
    The annual output value and profit tax is more
    than two billion yuan and three hundred million yuan, respectively.
    The export value is more than fifty million US dollars.)
\end{sent}

\subsubsection{Argument Extraction as a Sequence Labelling Problem}

We could formulate argument extraction as a sequence labelling problem.
Figure~\ref{i:rare-args} shows the arguments and the corresponding labels
for the four relations in Figure~\ref{i:cdtb-tree2}. Though we will
focus on explicit relations, the implicit relations are also shown
for completeness.
As we know the arguments will span over a continuous interval,
we use four labels for the EDUs: \textit{before}, \textit{start},
\textit{inside}, \textit{after}. Each argument is represented by
a \textit{start} followed by zero or more \textit{inside}s.

%i:rare-args
\input{figures/rare-args}

To obtain the correct labels, one must first segment the paragraph
into EDUs and then classify each EDU for a particular label.
However, as our goal is to extract the arguments, only the
argument boundaries must be determined as shown in
Figure~\ref{i:rare-bounds}. For a relation that has $N$ arguments,
there are $N+1$ boundaries. As long as we can identify these boundaries,
we could extract the arguments.


%i:rare-bounds
\input{figures/rare-boundaries}

Therefore, it's not required to obtain the EDUs as long as
we can divide the paragraph into segments that include all these boundaries.
In fact, even if we simply use the tokens as the segments, we could still satisfy
this requirement. One could improve the segmentation by making the observation
that the EDU boundaries in CDTB only occur with certain symbols that separate
phrases and sentences. The symbols include ”, …, ─, 、, 。, 」, ！, ，, ：, ； and ？.
We would segment the paragraph by these symbols, and solve the sequence labelling
problem on these segments.

\subsection{Determination of Argument Boundaries}

We use Conditional Random Fields (CRFs) to solve the sequence labelling problem.
The implementation we use is CRFsuite~\citep{CRFsuite}. When training, each explicit
relation with its corresponding labelling is used as an training case. When testing,
CRFs are used to label the segments for each connective we extracted. Therefore,
the same segments for a paragraph is labelled independently for each explicit relation
inside the paragraph. The resulting argument boundaries are used to extract the connective's
arguments. Although we have tried some rule-based techniques to adjust the argument boundaries
after they are extracted by utilizing their hierarchical relationships,
no significant improvement was observed.

\subsection{Features}

In this section, we discuss the features we use for each segment.

\subsubsection{CONTEXT}

This feature set is similar to the P \& N features, but we relax the
definition of self-category since most segments do not have a node that
dominates exactly its tokens. For each segment, we find the lowest node
that dominates all of the tokens in the segment. It may additionally
dominates some tokens from other segments. We define the self-category
to be the highest node that dominates exactly the same tokens as this node.
Afterwards, we find the categories of the parent, the left-sibling, and
the right-sibling of this node. The concatenation of the categories is
used as a binary feature as done by \cite{kong2014a}. We also found that
using the categories separately does not seem to improve the performance.

For example, in Figure~\ref{i:parse-segment} we show a sub- parsing tree for
(S~\ref{sent:rare-earth}), there are three segments as denoted by [].
The feature for the segment [二、 ] (two,) would be
\textbf{CONTEXT-QP-NP-Null-NP}.

%i:parse-segment
\input{figures/parse-segment}

\subsubsection{PATH}

The feature set is similar to the CON-NT-Path features from \cite{kong2014a}.
We use the path from the self-category of each connective component for the
current connective to the self-category of the segment as the feature. For
example, in Figure~\ref{i:parse-segment2} we show another sub- parsing tree.
The feature for the segment [现已建成包钢稀土一、] when labelling arguments
for the connective ``其中'' (among them) would be
\textbf{PATH-NP$\uparrow$IP$\uparrow$IP$\downarrow$IP}.

%i:parse-segment2
\input{figures/parse-segment2}
\subsubsection{POS}

This feature set contains the part-of-speech tags for all tokens in the segment.

\subsubsection{SUBJ}

The \textbf{SUBJ} feature is set if a segment contain a subject. Chinese Stanford
dependency parser~\citep{chang2009discriminative} is used to determine the
dependencies exist in each segment and we use the \textit{nsubj} dependency
for subject determination.


\subsubsection{ENDCHAR}

This feature is the last token in the segment (i.e. the symbol that separates
the current segment from the next).

\subsubsection{COMPONENT}

The feature set contains the information about each connective component for
the current connective.

\begin{enumerate}
\item COMPONENT-EXISTS: This feature is set if the current segment has
a connective component. It includes the following features:

\item COMPONENT-IS-: Additionally, we use the string for the component as a feature
if it exists in the segment. For example, if there is a connective component
``其中'' (among them), the feature would be \textbf{COMPONENT-IS-\zhbf{其中}}.

\item COMPONENT-BEGIN: This feature is set if there exists a component at
the beginning of this segment.

\item COMPONENT-END: This feature is set if there exists a component at
the end of this segment (i.e just before the symbol that separates the segment).

\item COMPONENT-ONLY: This feature is set if this segment contains only
the component and the separating symbol.

\item PREV-COMPONENT: This feature is set if the previous segment has
a connective component.

\item NEXT-COMPONENT: This feature is set if the next segment has
a connective component.
\end{enumerate}

These features only concern with the current connective being considered, so
the connective components for other explicit relations is not counted.

\subsubsection{LINK}

The feature set contains the linking directions a connective component
could be used if it exist in the given segment. The lexicon collected
from \cite{huang2014interpretation} is used for the linking directions.
For example, if the segment contains a connective component that is
annotated with all three linking directions, the features would be
\textbf{LINKING-FORWARD}, \textbf{LINKING-BACKWARD}, and \textbf{LINKING-COUPLE}.

\subsubsection{CONNECTIVE}

This feature set contains connective related information including the string
of the connective and the number of connective components. These features
do not depend on the segment itself. Therefore, each segment is given the
same features. For example, if we are finding the arguments for the connective
``虽-但'' (although-but), the features would be \textbf{CONNECTIVE-\zhbf{虽-但}}
and \textbf{CONNECTIVE-NUM-2}.

We have tried using the relation type as a feature as well, but it does not
improve the performance.

