%
%   Chapter Methods
%
%   Yong-Siang Shih
%   R.O.C.104.07
%
\chapter{Methods}
\label{c:method}

\section{Overview}

In this chapter, we discuss our system design for Chinese discourse analysis.
Our study mainly focus on the analysis of explicit relations.
The components for the pipeline system is shown in Figure~\ref{i:system}.

%i:system
\input{figures/system}

Firstly, we extract connective components candidates from each paragraph.
Secondly, an classifier is used to disambiguate between discourse and
non-discourse usages for these component candidates. We then try to identify
the correct connectives by solving the linking ambiguity between remaining
component candidates.
A different approach that eliminates non-discourse usages and disambiguate
linkings both on the connective level is also experimented. The alternative
approaches are illustrated in Figure~\ref{i:system-B}.
Finally, we determine the relation types for each explicit
connective and extract the arguments for each relation.

%i:system-B
\input{figures/system-B}


\section{Connective Candidate Extraction}

\subsection{Goal}

To identify the correct discourse connectives, we need to
extract connective candidates and distinguish between discourse
and non-discourse usages among these candidates. Therefore, we tackle the
candidate identification first. In particular, we focus on the identification
of potential connective components, as connective candidates can then be
generated by forming links between these component candidates.

\subsection{Extraction Methods}

Given the input of a Chinese paragraph and the connective component lexicon we collected,
we would like to extract all possible positions of component candidates.
The simplest method is to just use string matching with the connective
component lexicon to extract all possible instances. This yields
24,539 candidates, while only 2,131 of them are correct instances. An improvement
could be made by making the observation that many of the components only have
discourse meaning when they are paired with other components. So instead of matching
with the connective component lexicon, we use connective lexicon and only extract
a component candidate when it could independently form a single connective or pair
with other candidates to form a parallel connective.
This leaves us with 12,526 candidates.

One reason for many incorrect instances is that many characters used for connectives
appear in other unrelated words. For example, even though ``如'' (if) is a connective,
this character appears in an unrelated word ``如是说'' (says). To alleviate this problem,
Stanford Chinese segmenter~\citep{chang2008optimizing} is employed to segment paragraphs
into tokens. These token boundaries can be used as clues for eliminating spurious candidates.
In particular, we only extract a component when it can be composed by complete tokens.
For example, in (S~\ref{sent:keyissue}) we extract ``不是'' (not) and ``而是'' (but) as candidates
even though ``不'' (not) and ``是'' (is) are separated. Comparatively, in (S~\ref{sent:peace}),
the spurious connective component candidate ``和'' (and) is not extracted because it does
not satisfy a token boundary.

\begin{sent}{sent:keyissue}{}
    当前 / 经济 / 的 / 关键 / \underline{不} / \underline{是} / 争取 / 更 /
    高 / 的 / 增长 / 速度 / ， / \underline{而是} / 提高 / 效益 / 。
    (The key issue for current economy is not to pursue faster growth, but to
    increase productivity.)
\end{sent}

\begin{sent}{sent:peace}{}
    双方 / 表示 / 希望 / 在 / \underline{和}平 / 计划 / 的 / 基础 / 上 / 解决 / 问题 / 。
    (Both sides expressed the hope to solve the problem on the basis of the peace plan.)
\end{sent}

Using this procedure, only 7,649 component candidates are extracted, with 2,068 of 2,131 annotated
components recovered. These candidates could form 7,976 connective candidates, recovering
1,755 of 1,813 annotated connectives. Thus, 0.9704 and 0.9680 become upper bounds of recall
for the subsequent stages. Table~\ref{t:cand-extract} shows the comparison between different
extraction methods. The technique we use reduce spurious candidates substantially while
maintaining high recall.

%t:cand-extract
\input{tables/cand-extract}

\section{Discourse Usage Disambiguation}
\label{c:discourse-disambig}

\subsection{Goal}

We start by investigating the identification of connective components that have discourse functions
as most of the related works only deal with single connectives. Given the positions of all candidate
components, we would like to eliminate spurious candidates.

\subsection{Disambiguation on Component Level}
\label{c:discourse-disambig-component}


We use \textit{Scikit-Learn} library~\citep{scikit-learn} to carry out binary
classification between discourse and non-discourse usages for all candidates.
Features for each connective component are extracted and various
classifiers such as \textit{Logistic Regression} and
\textit{Support Vector Machine (SVM)} have been experimented, and the results
are reported in Chapter~\ref{c:exp}. The SVM included by Scikit-Learn is
a \textit{Python} wrapper for the popular \textit{LibSVM} library~\citep{CC01a}.
After we eliminate spurious components, we could form connective candidates
by linking the remaining components. Linking ambuirities must be solved to
identify the correct connective afterwards.


\subsection{Features for a Connective Component Candidate}
\label{s:comp-features}

In this section we discuss the features we propose.
Stanford POS tagger~\citep{toutanova2003feature} and Stanford Chinese
parser~\citep{levy2003harder} are used to create the POS tags and parsing tree
for each paragraph. When we extract features related to the parsing tree, we use
utilities provided by the \textit{Natural Language Toolkit}~\citep{BirdKleinLoper09}
to manipulate the parsing trees. The features we used for classification
are as follows.

\subsubsection{P\&N}

The feature set is a subset of the features from
\cite{pitler2009using}. It includes four binary features:
(1) the highest category that dominates exactly the component
itself in the parsing tree, which is called self-category, (2) the parent of the self-category,
(3) the left-sibling of the self-category, and (4) the right-sibling of the self-category.
Special null features are set when no such nodes exist. For example,
in Figure~\ref{i:parse-but}, there is no node that dominates exactly the
connective component ``却是'' (but), while in Figure~\ref{i:parse-therefore} the
self-category of ``因此'' (therefore) is ADVP. We have also experimented
with the full feature set from \cite{pitler2009using}, but the performance
does not increase.

%i:parse-but
%i:parse-therefore
\input{figures/parse}

\subsubsection{POS}

The feature set contains three types of binary features:
(1) part-of-speech tags for all tokens that constitute the connective component,
(2) part-of-speech tag of the token to the left of the component, and
(3) part-of-speech tag of the token to the right of the component. For example,
in (S~\ref{sent:sovereignty}) the features for ``意味着'' (meaning) are
\textbf{POS-left-PU}, \textbf{POS-right-PN}, \textbf{POS-self-VV}, and
\textbf{POS-self-AS}.

\begin{sent}{sent:sovereignty}{}
    一百二十七/CD 位/M 委员/NN 一致/AD 通过/VV ，/PU \underline{意味/VV 着/AS} 大家/PN 对/P
    恢复/VV 行使/VV 香港/NR 主权/NN \underline{后/LC} 的/DEG 政治/NN 架构/NN 重建/NN ，/PU
    有/VE 着/AS 高度/JJ 的/DEG 共识/NN 。/PU
    (All 127 committee members agreed, meaning that everyone reaches a consensus about
    the reconstruction of political structure after resuming exercise of sovereignty over Hong Kong.)
\end{sent}

\subsubsection{NUM}

The feature set contains three numerical features.

\begin{enumerate}
    \item The number of detected connective candidates the component involves. The larger
        the number means that the component candidate can form more plausible connective
        candidates by linking with other component candidates.
    \item The distance from the connective component to a separating element on the left.
    \item The distance from the connective component to a
        separating element on the right.
\end{enumerate}

Distance is measured by adding 1 to the number of tokens
between the separating element and the nearest token of the component.
The separating elements include the symbols ``！？：；，。''.
When no such symbol is found, we use the distance to paragraph boundary.

For example, in (S~\ref{sent:sovereignty}) the features for “意味着” (meaning) are \textbf{NUM-ambiguity=1},
\textbf{NUM-left=1}, and \textbf{NUM-right=12}. As there is only one connective candidate that has the
component “意味着” (meaning), the linking ambiguity for the component is 1.
We normalize the numerical features by scaling each to zero mean and unit variance.


\subsubsection{VECTOR}

This feature set is built using word embeddings we created.
We have tried creating 400-dimensional embeddings by GloVe tool~\citep{pennington2014glove}
and word2vec tool~\citep{mikolov2013efficient,mikolov2013distributed}.

The vectors are used to construct three features: (1) the averaged vectors
for the tokens that constitute the connective component, (2) the vector for the
token to the left and (3) the vector for the token to the right. Zero-valued vector is used
when the vector does not exist.

In total, it's a 1200-dimensional vector when the 400-dimensional embeddings are used.
The dimensionality increases proportionally if we combine different vectors produced by
different tools.

\subsection{Disambiguation on Connective Level}
\label{c:discourse-disambig-connective}


Rather than disambiguate each component candidate, we could also treat each connective
candidate as an instance and tackle discourse usage disambiguation on the connective
level. All possible connective candidates are  generated by linking components with
each other. Features for each connective candidate are extracted and binary
classification can be carried out to identify discourse usages.
Since there are more spurious connectives than real connectives, we balance the
data set by oversampling the correct instances for three times. To evaluate on
component level, we take the union of the components for all connective as the result.

Suppose we can do the classification perfectly, we would have already
solved the linking ambiguities as only the correct connectives remain.
However, there may still exist some spurious candidates, some of which are
caused by incorrect component candidates, others caused by
incorrect linkings.
In particular, some overlapped connective candidates that share the same
connective component candidate remain. By resolving the linking
ambiguities, we will have more capacity to identify the correct connectives. This
work is described in Section~\ref{c:linking-disambig}.

\subsection{Features for a Connective Candidate}
\label{c:connective-features}

The feature sets for each connective candidate are similar to those in the previous
experiment. As a connective candidate is composed of one or more component candidates,
we extend some of the features to utilize linking relationships between the
components.

\subsubsection{P\&N}

The feature set contains the union of self-category, parent-category, left-sibling,
and right-sibling as defined in Section~\ref{s:comp-features} for each component.
For example, in Figure~\ref{i:parse-although}, the features for ``虽'' (although) are
\textbf{PN-self-ADVP}, \textbf{PN-parent-VP}, \textbf{PN-left-Null}, and \textbf{PN-right-ADVP}, while
the features for ``但'' (but) are \textbf{PN-self-ADVP}, \textbf{PN-parent-IP}, \textbf{PN-left-Null},
and \textbf{PN-right-NP}. Therefore, the combined features for ``虽-但'' (although-but) are
\textbf{PN-self-ADVP}, \textbf{PN-parent-VP}, \textbf{PN-parent-IP}, \textbf{PN-left-Null},
\textbf{PN-right-ADVP}, and \textbf{PN-right-NP}.


%i:parse-although
\input{figures/parse-although}


\subsubsection{POS}

The feature set contains three types of binary features:
(1) part-of-speech tags for all tokens that constitute the connective components
of the connective candidate,
(2) part-of-speech tag of the token to the left of each component, and
(3) part-of-speech tag of the token to the right of each component. For example,
in (S~\ref{sent:crisis}), the features for ``虽-但'' (although-but) are
\textbf{POS-left-NR}, \textbf{POS-right-AD}, \textbf{POS-self-CS},
\textbf{POS-left-PU}, \textbf{POS-right-JJ}, and \textbf{POS-self-AD}.

\begin{sent}{sent:crisis}{}
    东亚/NR 诸国/NN 宛如/VV 骨牌/NN 般/VA 连番/AD 倒下/VV 。/PU
    台湾/NR \underline{虽/CS} 只/AD 遭/VV 风暴/NN 边缘/NN 扫过/VV ，/PU
    \underline{但/AD} 後续/JJ 冲击/NN 将/AD 一/CD 波波/M 涌到/VV 。/PU
    (East Asian countries fell like dominoes.
    Although Taiwan is only swept by the edge of the storm,
    the follow-up impact would come.
\end{sent}

\subsubsection{NUM}

The feature set contains the length of the connective as a set
of binary features. The length is defined as the number of components
that it contains.
In additional, there are seven numerical features.
\begin{enumerate}
    \item The number of overlapped connective candidates. Any connective
        candidates that share a token with any of the components for the current
        candidate is considered as overlapped.
    \item The number of connective candidates that cross any
        components of the current connective. A connective crosses a component
        if the component is between any two components of the connective.
    \item The distance between the leftmost and the rightmost tokens of the connective.
        The distance is measured by tokens as before.
    \item The geometric mean of distances between all neighboring connective components for
        The current connective candidates.
    \item The distances from the leftmost connective component to a separating element on the left.
    \item The distance from the rightmost connective component to a separating element on the right.
    \item The minimum distance from any separating element to any connective component.
\end{enumerate}

The separating elements include the symbols ``！？：；，。''.
When no such symbol is found, we use the distance to paragraph boundary.
We normalize the numerical features by scaling each to zero mean and unit variance.

\subsubsection{VECTOR}

This feature set is built using word embeddings as before, averaging over all components.
In particular, it contains three vectors: (1) the averaged vectors
for the all tokens that constitute the connective components, (2) the averaged vector for the
token to the left of each component and (3) the averaged vector for the token to the
right of each component. Zero-valued vector is used when the vector does not exist.

\section{Connective Linking Disambiguation}
\label{c:linking-disambig}

\subsection{Goal}

Given all connective component candidates, we could construct all possible
connective candidates formed by these component. Many of these
candidates are incorrect. In particular, a component may involves in
many different connective candidates by linking with different components,
but only one of them could be correct. Moreover, some remaining component
candidates may not actually have discourse usage, meaning that none of
the connectives it forms is correct. We would like to extract the
correct connectives from the paragraph despite the imperfect discourse usage
disambiguation.

\subsection{A Pipeline Approach with Discourse Usage Disambiguation on Component Level}
\label{c:pipeline1}

After we have eliminated the non-discourse connective components as described
in Section~\ref{c:discourse-disambig-component}, we could
deal with liking ambiguities by determining how to correctly pair these
components together. We extract all possible connective candidates by
linking these components. We treat each candidate as an instance to
extract related features as described in Section~\ref{c:connective-features}.
Afterwards, logistic regression is used to obtained the likelihood for an
candidate to be a correct connective.

The likelihood is used as scores, and connective candidates are greedily accepted
ordered by their lengths (the number of components they have) and their scores.
Ties are broken by the positions of the connectives. Candidates that have
leftmost positions are accepted first.

We reject a connective candidate when it share the same connective component
with the already accepted connectives. The reason to use length as an ordering
criteria is due to the observation that long connectives appear rarely
and when they do appear, it's often the case that it's a real connective.

After this procedure, additional connective components may have been eliminated if
they are unable to form a link with other candidates and is not a single connective.
Therefore, we also evaluate whether this procedure could improve the performance
for discourse usage identification in component level.


\subsection{A Pipeline Approach with Discourse Usage Disambiguation on Connective Level}
\label{c:pipeline2}

Rather than eliminating non-discourse connective components first and
dealing with linking disambiguation for overlapped connectives later,
we could also try to solve them both on connective level as linking may
provide additional clues for detecting discourse usage. 

In Section~\ref{c:discourse-disambig-component}, we generate all possible
connective candidates and eliminate ones that are unlikely to be correct.
In the process, we eliminate some non-discourse candidates and some incorrect
linkings.

Afterwards, there remain some connective candidates that overlap with each other.
That is, they share the same connective component. So we greedily accept connective
candidates ordered by their lengths and then their scores, while rejecting
overlapped ones as described in Section~\ref{c:pipeline1}.

Likewise, we also evaluate on component level to see if this increase
the performance for discourse usage detection.

\section{Relation Type Disambiguation}

After the connectives are extracted, the relation type the connective represent
must be determined. We use the features in Section~\ref{c:connective-features}
and additionally we use the identity of the connective as a feature as
it provides strong clues for it's relation type.


\section{Connective Argument Extraction}

\subsection{Goal}

\subsection{Argument Extraction as Sequence Labeling}

\subsection{Features}

\subsubsection{CONTEXT}

This feature set is the combination of the self-category, parent-category, left-sibling,
and right-sibling of the segment. Since most segment do not have a node that dominates
exactly its tokens. We relax the definition of self-category to be the highest node that
dominates all of the tokens in the segment besides other tokens. Since the root node would
always be the self-category with this definition, we add another extr
